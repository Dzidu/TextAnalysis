count(url, word, sort = TRUE) %>%
bind_tf_idf(word, url, n) %>%
select(url, word, tf) %>%
spread(word, tf, fill = 0)
dtm
topic_minutes <- LDA(dtm,
k = 4, # Liczba tematów
control = list(seed = 1234)) # losowanie punktów startowych
topic_minutes
#dtm
dtm <- articles_grouped[,-2:-5] %>%
unnest_tokens(word, full_body_lemmatized) %>%
anti_join(stop_words) %>%
filter(nchar(word) > 2) %>%
count(url, word, sort = TRUE) %>%
bind_tf(word, url, n) %>%
select(url, word, tf) %>%
spread(word, tf, fill = 0)
dtm
#dtm
dtm <- articles_grouped[,-2:-5] %>%
unnest_tokens(word, full_body_lemmatized) %>%
anti_join(stop_words) %>%
filter(nchar(word) > 2) %>%
count(url, word, sort = TRUE) %>%
bind_tf_idf(word, url, n) %>%
select(url, word, tf) %>%
spread(word, tf, fill = 0)
#dtm
dtm <- articles_grouped[,-2:-5] %>%
unnest_tokens(word, full_body_lemmatized) %>%
anti_join(stop_words) %>%
filter(nchar(word) > 2) %>%
count(url, word, sort = TRUE) %>%
dtm <- DocumentTermMatrix(dtm,
control = list(
weighting = weightTf
))
#dtm
dtm <- articles_grouped[,-2:-5] %>%
unnest_tokens(word, full_body_lemmatized) %>%
anti_join(stop_words) %>%
filter(nchar(word) > 2)
View(dtm)
dtm <- DocumentTermMatrix(dtm,
control = list(
weighting = weightTf
))
dtm_matrix <- as.matrix(dtm)
View(dtm_matrix)
#dtm
dtm <- articles_grouped[,-1:-5] %>%
unnest_tokens(word, full_body_lemmatized) %>%
anti_join(stop_words) %>%
filter(nchar(word) > 2)
dtm <- DocumentTermMatrix(dtm,
control = list(
weighting = weightTf
))
dtm_matrix <- as.matrix(dtm)
topic_minutes <- LDA(dtm,
k = 4, # Liczba tematów
control = list(seed = 1234)) # losowanie punktów startowych
topic_minutes
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
arrange(topic, -beta) # ustalamy wg największych wartości beta
mi_topics <- tidy(topic_minutes, matrix = "beta")
mi_topics
mi_topics <- tidy(topic_minutes, matrix = "beta")
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(hunspell)
library(rvest)
library(stringr)
library(RSelenium)
library(wordcloud)
library(wordcloud2)
library(plotrix)
library(lubridate)
dtm_matrix <- as.matrix(dtm)
topic_minutes <- LDA(dtm,
k = 4, # Liczba tematów
control = list(seed = 1234)) # losowanie punktów startowych
topic_minutes
View(topic_minutes)
mi_topics <- tidy(topic_minutes, matrix = "beta")
install.packages("reshape2")
library(reshape2)
mi_topics <- tidy(topic_minutes, matrix = "beta")
mi_topics
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
arrange(topic, -beta) # ustalamy wg największych wartości beta
top_terms
top_terms %>%
mutate(term = reorder(term, beta)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
View(articles_grouped)
corpus <- Corpus(VectorSource(articles_grouped)$full_body_lemmatized))
corpus <- Corpus(VectorSource(articles_grouped)$full_body_lemmatized)
dtm_matrix <- as.matrix(dtm)
View(dtm_matrix)
#dtm
dtm <- articles_grouped[,-1:-5] %>%
unnest_tokens(word, full_body_lemmatized, drop = FALSE) %>%
anti_join(stop_words) %>%
filter(nchar(word) > 2)
dtm <- DocumentTermMatrix(dtm,
control = list(
weighting = weightTf
))
View(dtm)
#dtm
dtm <- articles_grouped[,-1:-5] %>%
unnest_tokens(word, full_body_lemmatized, drop = FALSE) %>%
anti_join(stop_words) %>%
filter(nchar(word) > 2)
dtm <- DocumentTermMatrix(dtm,
control = list(
weighting = weightTf
))
dtm_matrix <- as.matrix(dtm)
View(dtm)
View(word_tokens)
View(articles_grouped)
View(word_tokens)
word_top <- word_tokens[,-1:-3] %>%
count(month, word, sort = T) %>%
group_by(month) %>%
top_n(15, n) %>%
ungroup() %>%
mutate(word = reorder(word, n))
ggplot(word_top, aes(x = reorder_within(word, n, month), y = n, fill = month)) +
geom_col(show.legend = F) +
coord_flip() +
facet_wrap(~month, scales ="free_y" ) +
scale_x_reordered()
View(word_tokens)
dtm <- DocumentTermMatrix(word_tokens[,-c(1,2,3,5)],
control = list(
weighting = weightTf))
dtm_matrix <- as.matrix(dtm)
View(dtm_matrix)
#dtm
articlesCorpus <- Corpus(VectorSource(word_token))
#dtm
articlesCorpus <- Corpus(VectorSource(word_tokens[,-c(1,2,3,5)))
#dtm
articlesCorpus <- Corpus(VectorSource(word_tokens[,-c(1,2,3,5))
#dtm
word_tokens_long<-word_tokens[,-c(1,2,3,5)
#dtm
word_tokens_long<-word_tokens[,-c(1,2,3,5)]
#dtm
word_tokens_long<-word_tokens[,-c(1,2,3,5)]
View(word_tokens_long)
word_tokens_long_corpus <- Corpus(VectorSource(word_tokens_long))
dtm <- DocumentTermMatrix(word_tokens_long_corpus,
control = list(
weighting = weightTf))
dtm_matrix <- as.matrix(dtm)
View(word_tokens_long_corpus)
dtm <- DocumentTermMatrix(word_tokens_long_corpus,
control = list(weighting = weightTf))
topic_minutes <- LDA(dtm,
k = 4, # Liczba tematów
control = list(seed = 1234)) # losowanie punktów startowych
topic_minutes
mi_topics <- tidy(topic_minutes, matrix = "beta")
mi_topics
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
arrange(topic, -beta) # ustalamy wg największych wartości beta
top_terms
top_terms %>%
mutate(term = reorder(term, beta)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
View(word_tokens_long)
word_tokens_long_corpus <- Corpus(DFSource(word_tokens_long))
word_tokens_long_corpus <- Corpus(DataFrameSource(word_tokens_long))
#dtm
word_tokens_long<-word_tokens[,-c(1,2,3,5)]
word_tokens_long_corpus <- Corpus(VectorSource(word_tokens_long$word))
dtm <- DocumentTermMatrix(word_tokens_long_corpus,
control = list(weighting = weightTf))
dtm_matrix <- as.matrix(dtm)
topic_minutes <- LDA(dtm,
k = 4, # Liczba tematów
control = list(seed = 1234)) # losowanie punktów startowych
topic_minutes
mi_topics <- tidy(topic_minutes, matrix = "beta")
mi_topics
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
arrange(topic, -beta) # ustalamy wg największych wartości beta
top_terms
top_terms %>%
mutate(term = reorder(term, beta)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
ggplot(word_top, aes(x = reorder_within(word, n, month), y = n, fill = month)) +
geom_col(show.legend = F) +
coord_flip() +
facet_wrap(~month, scales ="free_y" ) +
scale_x_reordered()
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term = reorder_within(term, beta, topic), y = beta, fill = topic)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
ggplot(aes(term = reorder_within(term, beta, topic), y = beta, fill = topic)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(hunspell)
library(rvest)
library(stringr)
library(RSelenium)
library(wordcloud)
library(wordcloud2)
library(plotrix)
library(lubridate)
library(topicmodels)
library(reshape2)
base_url_pt1 <- "https://cryptonews.net/?page="
n_index_pages <- 25
get_article_list_from_page <- function(page_no) {
page_url <- paste0(base_url_pt1, page_no)
page <- read_html(page_url)
links <- page %>%
html_node("section") %>%
html_nodes("div.desc.col-xs")
articles_tmp <- tibble(link=paste0("https://cryptonews.net",url = links %>% html_node("a") %>% html_attr("href")),
title =  links %>% html_node("a") %>% html_text())
return(articles_tmp)
}
article_links <- tibble()
for(i in 25:n_index_pages) {
article_links_tmp <- get_article_list_from_page(i)
article_links <- bind_rows(article_links, article_links_tmp)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
print(i)
}
rm(article_links_tmp, i)
webpage <- read_html(encoding = "ISO_8859-2", 'https://cryptonews.net/news/other/21113274/')
webpage
results <- webpage %>% html_nodes(".short-desc")
results
safe_read_html <- safely(read_html)
get_article <- function(art_url) {
try(page <- safe_read_html(art_url, encoding = "iso-8859-2"))
if(is.null(page$result)) {
return(tibble())
}
page <- page$result
#author <- page %>% html_node('span.source-host') %>% html_text() %>% trimws()
date <- page %>% html_node("span.datetime.flex.middle-xs") %>% html_text() %>% trimws() %>% dmy_hm()
title <- page %>%html_node("h1") %>% html_text() %>% trimws()
lead <- page %>% html_node("div.news-item.detail.content_text") %>% html_node("p") %>% html_text() %>% trimws()
body <- page %>% html_node("div.news-item.detail.content_text") %>% html_nodes("p") %>% html_text() %>% trimws()
article <- tibble(url = art_url, title = title, date = date, lead = lead, body = body,)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
return(article)
}
articles <- article_links %>%
rowwise() %>%
do(get_article(.$link)) %>%
bind_rows() %>%
ungroup()
articles_grouped <- articles %>%
group_by(url,title,date,lead )  %>%
summarise(full_body = paste(body, collapse = " "))
clean.corpus <- function(corpus){
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords('en'))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(lemmatize_strings))
return(corpus)
}
articles_grouped$full_body_lemmatized <- NA
for(i in 1:nrow(articles_grouped)) {
articles_long <- articles_grouped[i,"full_body"] %>%
unnest_tokens(word,full_body )
articlesCorpus <- Corpus(VectorSource(articles_long))
articlesCorpus <- clean.corpus(articlesCorpus)
articles_grouped[i,"full_body_lemmatized"] <- data.frame(text = sapply(articlesCorpus, as.character), stringsAsFactors = FALSE)
}
word_tokens <- articles_grouped %>%
unnest_tokens(word, full_body_lemmatized, drop = FALSE) %>%
select(-full_body_lemmatized) %>%
filter(!str_detect(word, '\\d')) %>%
filter(nchar(word) > 2)
#chmura słow
word_count <- word_tokens %>%
count(word)
par(mar = c(0, 0, 0, 0))
wordcloud(word_count$word,
word_count$n, max.words = 100,
rot.per = 0.35, colors = brewer.pal(12,"Paired"))
word_tokens$month <- month(ymd_hms(word_tokens$date))
word_top <- word_tokens[,-1:-3] %>%
count(month, word, sort = T) %>%
group_by(month) %>%
top_n(15, n) %>%
ungroup() %>%
mutate(word = reorder(word, n))
ggplot(word_top, aes(x = reorder_within(word, n, month), y = n, fill = month)) +
geom_col(show.legend = F) +
coord_flip() +
facet_wrap(~month, scales ="free_y" ) +
scale_x_reordered()
#dtm
word_tokens_long<-word_tokens[,-c(1,2,3,5)]
word_tokens_long_corpus <- Corpus(VectorSource(word_tokens_long$word))
dtm <- DocumentTermMatrix(word_tokens_long_corpus,
control = list(weighting = weightTf))
topic_minutes <- LDA(dtm,
k = 4,
control = list(seed = 2137692115))
mi_topics <- tidy(topic_minutes, matrix = "beta")
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
arrange(topic, -beta) # ustalamy wg największych wartości beta
top_terms %>%
mutate(term = reorder(term, beta)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = topic)) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free_y") +
coord_flip()
top_terms %>%
mutate(topic = as.factor(topic),
term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(topic = as.factor(topic),
term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free_y") +
coord_flip()
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
#arrange(topic, -beta) # ustalamy wg największych wartości beta
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
#arrange(topic, -beta) # ustalamy wg największych wartości beta
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup()# %>%
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta)) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
arrange(topic, -beta) # ustalamy wg największych wartości beta
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(topic = as.factor(topic),
term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(topic = as.factor(topic),
term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = topic)) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(topic = as.factor(topic),
term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = topic)) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free_y") +
coord_flip()
top_terms %>%
mutate(topic = as.factor(topic),
term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = topic)) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free_y") +
coord_flip()+
scale_x_reordered()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
top_terms <- mi_topics %>%
group_by(topic) %>% # grupujemy wg tematów
top_n(10, beta) %>% # 10 największych prawdopodobieństw beta w każdym temacie
ungroup() %>%
arrange(topic, -beta) # ustalamy wg największych wartości beta
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>% # układa wyrażenia wg malejących wartości beta
ggplot(aes(term, beta, fill = factor(topic))) + # tematy będą zaznaczone innymi kolorami
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
library(textdata)
install.packages("textdata")
library(textdata)
