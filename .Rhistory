do(get_article(.$link)) %>%
bind_rows() %>%
ungroup()
get_article <- function(art_url) {
try(page <- safe_read_html(art_url, encoding = "iso-8859-2"))
if(is.null(page$result)) {
return(tibble())
}
page <- page$result
author <- page %>% html_node('span.source-host') %>% html_text() %>% trimws()
date <- page %>% html_node("span.datetime.flex.middle-xs") %>% html_text() %>% trimws() %>% dmy_hm()
title <- page %>%html_node("h1") %>% html_text() %>% trimws()
lead <- page %>% html_node("p") %>% html_text() %>% trimws()
body <- page %>% html_node("div.news-item.detail.content_text") html_node("p") %>% html_text() %>% trimws()
article <- tibble(url = art_url, title = title, author = author, date = date, lead = lead, body = body,)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
return(article)
}
get_article <- function(art_url) {
try(page <- safe_read_html(art_url, encoding = "iso-8859-2"))
if(is.null(page$result)) {
return(tibble())
}
page <- page$result
author <- page %>% html_node('span.source-host') %>% html_text() %>% trimws()
date <- page %>% html_node("span.datetime.flex.middle-xs") %>% html_text() %>% trimws() %>% dmy_hm()
title <- page %>%html_node("h1") %>% html_text() %>% trimws()
lead <- page %>% html_node("p") %>% html_text() %>% trimws()
body <- page %>% html_node("div.news-item.detail.content_text") %>% html_node("p") %>% html_text() %>% trimws()
article <- tibble(url = art_url, title = title, author = author, date = date, lead = lead, body = body,)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
return(article)
}
articles <- article_links %>%
rowwise() %>%
do(get_article(.$link)) %>%
bind_rows() %>%
ungroup()
get_article <- function(art_url) {
try(page <- safe_read_html(art_url, encoding = "iso-8859-2"))
if(is.null(page$result)) {
return(tibble())
}
page <- page$result
author <- page %>% html_node('span.source-host') %>% html_text() %>% trimws()
date <- page %>% html_node("span.datetime.flex.middle-xs") %>% html_text() %>% trimws() %>% dmy_hm()
title <- page %>%html_node("h1") %>% html_text() %>% trimws()
lead <- page %>% html_node("p") %>% html_text() %>% trimws()
body <- page %>% html_node("div.news-item.detail.content_text") %>% html_nodes("p") %>% html_text() %>% trimws()
article <- tibble(url = art_url, title = title, author = author, date = date, lead = lead, body = body,)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
return(article)
}
articles <- article_links %>%
rowwise() %>%
do(get_article(.$link)) %>%
bind_rows() %>%
ungroup()
articles_grouped <- articles %>% group_by(url, title) %>%
articles_grouped <- articles %>% group_by(url, title)
articles_grouped <- articles %>% group_by(url, title) %>% summarise(across(body, sum))
articles_grouped <- articles %>% group_by(url, title) %>% summarise(across(body, mutate))
articles_grouped <- articles %>% group_by(url, title, author, date) %>% summarise(across(body, unite))
articles_grouped <- articles %>% group_by(url, title, author, date) %>% summarise(across(body, paste))
View(articles_grouped)
articles_grouped <- articles %>% group_by(url, title, author, date) %>% summarise(paste(body, sep = " "))
articles_grouped <- articles %>% group_by(url, title, author, date) %>% reframe(paste(body, sep = " "))
View(articles)
articles_grouped <- articles %>%
group_by(url,title,author,date,lead )
articles_grouped <- articles %>%
group_by(url,title,author,date,lead )  %>%
summarise(full_body = paste(body, collapse = " "))
length(unique(articles$url))
length(unique(articles$title))
length(unique(articles$author))
length(unique(articles$lead))
articles_grouped <- articles %>%
group_by(url )  %>%
summarise(full_body = paste(body, collapse = " "))
articles_grouped <- articles %>%
group_by(url,title,author,date,lead )  %>%
summarise(full_body = paste(body, collapse = " "))
load("C:/Users/Dom/Desktop/R/Rcw/R projekt/.RData")
load("C:/Users/Dom/Documents/GitHub/R projekt/.RData")
getwd()
setwd('C:/Users/Dom/Desktop/R/Rcw/Text Analysis/TextAnalysis')
saveRDS(articles_grouped, file = "articles_full.RDS")
View(articles_grouped)
articles_grouped[1][6]
articles_grouped[1][full_body]
articles_grouped[1,full_body]
articles_grouped.at[1,full_body]
articles_grouped[1][full_body]
articles_grouped[1,"full_body]
articles_grouped[1,"full_body"]
articles_grouped[1]
articles_grouped[1,2]
articles_grouped[1,6]
articles_grouped["full_body"]
articles_long <- articles_grouped %>%
unnest_tokens(word,full_body)
library(janeaustenr)
articles_long <- articles_grouped %>%
unnest_tokens(word,full_body)
library(tidytext)
library(tm)
library(stopwords)
library(textstem)
library(hunspell)
articles_long <- articles_grouped %>%
unnest_tokens(word,full_body)
articles_long <- articles_grouped["full_body"] %>%
unnest_tokens(word,full_body)
articles_long <- articles_grouped["full_body"]
View(articles_long)
articles_text <- articles_grouped["full_body"]
articles_text %>%
unnest_tokens(word,full_body)
articles_text %>%
tidytext::unnest_tokens(word,full_body)
articles_full <- read.table("C:/Users/Dom/Desktop/R/Rcw/Text Analysis/TextAnalysis/articles_full.RDS", header=TRUE, quote="\"")
View(articles_full)
base_url_pt1 <- "https://cryptonews.net/?page="
n_index_pages <- 25
get_article_list_from_page <- function(page_no) {
page_url <- paste0(base_url_pt1, page_no)
page <- read_html(page_url)
links <- page %>%
html_node("section") %>%
html_nodes("div.desc.col-xs")
articles_tmp <- tibble(link=paste0("https://cryptonews.net",url = links %>% html_node("a") %>% html_attr("href")),
title =  links %>% html_node("a") %>% html_text())
return(articles_tmp)
}
article_links <- tibble()
for(i in 25:n_index_pages) {
article_links_tmp <- get_article_list_from_page(i)
article_links <- bind_rows(article_links, article_links_tmp)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
print(i)
}
rm(article_links_tmp, i)
library(tidyverse)
library(tidytext)
library(tm)
library(stopwords)
library(textstem)
library(hunspell)
library(rvest)
library(stringr)
library(lubridate)
library(purrr)
library(dplyr, warn.conflicts = T)
library(RSelenium)
base_url_pt1 <- "https://cryptonews.net/?page="
n_index_pages <- 25
get_article_list_from_page <- function(page_no) {
page_url <- paste0(base_url_pt1, page_no)
page <- read_html(page_url)
links <- page %>%
html_node("section") %>%
html_nodes("div.desc.col-xs")
articles_tmp <- tibble(link=paste0("https://cryptonews.net",url = links %>% html_node("a") %>% html_attr("href")),
title =  links %>% html_node("a") %>% html_text())
return(articles_tmp)
}
article_links <- tibble()
for(i in 25:n_index_pages) {
article_links_tmp <- get_article_list_from_page(i)
article_links <- bind_rows(article_links, article_links_tmp)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
print(i)
}
rm(article_links_tmp, i)
webpage <- read_html(encoding = "ISO_8859-2", 'https://cryptonews.net/news/other/21113274/')
webpage
results <- webpage %>% html_nodes(".short-desc")
results
safe_read_html <- safely(read_html)
get_article <- function(art_url) {
try(page <- safe_read_html(art_url, encoding = "iso-8859-2"))
if(is.null(page$result)) {
return(tibble())
}
page <- page$result
author <- page %>% html_node('span.source-host') %>% html_text() %>% trimws()
date <- page %>% html_node("span.datetime.flex.middle-xs") %>% html_text() %>% trimws() %>% dmy_hm()
title <- page %>%html_node("h1") %>% html_text() %>% trimws()
lead <- page %>% html_node("div.news-item.detail.content_text") %>% html_node("p") %>% html_text() %>% trimws()
body <- page %>% html_node("div.news-item.detail.content_text") %>% html_nodes("p") %>% html_text() %>% trimws()
article <- tibble(url = art_url, title = title, author = author, date = date, lead = lead, body = body,)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
return(article)
}
articles <- article_links %>%
rowwise() %>%
do(get_article(.$link)) %>%
bind_rows() %>%
ungroup()
articles_grouped <- articles %>%
group_by(url,title,author,date,lead )  %>%
summarise(full_body = paste(body, collapse = " "))
articles_text <- articles_grouped["full_body"]
articles_text %>%
tidytext::unnest_tokens(word,full_body)
View(articles_text)
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body)
View(articles_long)
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word')
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]'))
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body)
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]'))
View(articles_long)
View(articles_grouped)
View(articles_text)
library(tidyverse)
library(tidytext)
library(tm)
library(stopwords)
library(textstem)
library(hunspell)
library(rvest)
library(stringr)
library(lubridate)
library(purrr)
library(dplyr, warn.conflicts = T)
library(RSelenium)
base_url_pt1 <- "https://cryptonews.net/?page="
n_index_pages <- 25
get_article_list_from_page <- function(page_no) {
page_url <- paste0(base_url_pt1, page_no)
page <- read_html(page_url)
links <- page %>%
html_node("section") %>%
html_nodes("div.desc.col-xs")
articles_tmp <- tibble(link=paste0("https://cryptonews.net",url = links %>% html_node("a") %>% html_attr("href")),
title =  links %>% html_node("a") %>% html_text())
return(articles_tmp)
}
article_links <- tibble()
for(i in 25:n_index_pages) {
article_links_tmp <- get_article_list_from_page(i)
article_links <- bind_rows(article_links, article_links_tmp)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
print(i)
}
rm(article_links_tmp, i)
webpage <- read_html(encoding = "ISO_8859-2", 'https://cryptonews.net/news/other/21113274/')
webpage
results <- webpage %>% html_nodes(".short-desc")
results
safe_read_html <- safely(read_html)
get_article <- function(art_url) {
try(page <- safe_read_html(art_url, encoding = "iso-8859-2"))
if(is.null(page$result)) {
return(tibble())
}
page <- page$result
author <- page %>% html_node('span.source-host') %>% html_text() %>% trimws()
date <- page %>% html_node("span.datetime.flex.middle-xs") %>% html_text() %>% trimws() %>% dmy_hm()
title <- page %>%html_node("h1") %>% html_text() %>% trimws()
lead <- page %>% html_node("div.news-item.detail.content_text") %>% html_node("p") %>% html_text() %>% trimws()
body <- page %>% html_node("div.news-item.detail.content_text") %>% html_nodes("p") %>% html_text() %>% trimws()
article <- tibble(url = art_url, title = title, author = author, date = date, lead = lead, body = body,)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
return(article)
}
articles <- article_links %>%
rowwise() %>%
do(get_article(.$link)) %>%
bind_rows() %>%
ungroup()
articles_grouped <- articles %>%
group_by(url,title,author,date,lead )  %>%
summarise(full_body = paste(body, collapse = " "))
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]'))
View(articles_long)
articles_long <- articles_long %>%
mutate(check = hunspell_check(word),
suggest = ifelse(!check, hunspell_suggest(word), word))
articles_long$word2 <- sapply(articles_long$suggest, function(x) x[1])
articles_long
View(articles_long)
View(articles_long)
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]'))
articles_long <- articles_long %>%
mutate(word2 = hunspell_stem(word))
articles_long$word2 <- sapply(articles_long$word2, function(x) x[1])
articles_long <- articles_long %>%
mutate(word3 = ifelse(is.na(word2), word, word2))
View(articles_long)
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]')) %>%
filter(str_detect(word, '[A-z]'))
articles_long <- articles_long %>%
mutate(word2 = hunspell_stem(word))
articles_long$word2 <- sapply(articles_long$word2, function(x) x[1])
articles_long <- articles_long %>%
mutate(word3 = ifelse(is.na(word2), word, word2))
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]')) %>%
filter(str_detect(word, '[A-z]\w'))
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]')) %>%
filter(str_detect(word, '[A-z]\\w'))
articles_long <- articles_long %>%
mutate(word2 = hunspell_stem(word))
articles_long$word2 <- sapply(articles_long$word2, function(x) x[1])
articles_long <- articles_long %>%
mutate(word3 = ifelse(is.na(word2), word, word2))
articles_long <- articles_grouped["full_body"] %>%
tidytext::unnest_tokens(word,full_body) %>%
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]')) %>%
filter(str_detect(word, '[A-z]')) %>%
filter(str_detect(word, '\\w'))
articles_long <- articles_long %>%
mutate(word2 = hunspell_stem(word))
articles_long$word2 <- sapply(articles_long$word2, function(x) x[1])
articles_long <- articles_long %>%
mutate(word3 = ifelse(is.na(word2), word, word2))
articles_long <- articles_grouped["full_body"] %>%
unnest_tokens(word,full_body) %>%
articles_long <- articles_grouped["full_body"] %>%
unnest_tokens(word,full_body) %>%
View(articles_long)
articles_long <- articles_grouped["full_body"] %>%
unnest_tokens(word,full_body)
View(articles_long)
#articles_long <- articles_long %>%
#  mutate(word3 = ifelse(is.na(word2), word, word2))
dfCorpus <- Corpus(articles_long)
dfCorpus <- Corpus(articles_long)
dfCorpus <- Corpus(DirSource(paste0(getwd(),'/Lekcja 5/Dane/minutes_nbp_en_text_long/')))
dfCorpus <- Corpus(articles_long)
dfCorpus <- Corpus(articles_long, text_field = "word")
dfCorpus <- Corpus(articles_long,"word")
text_field =
dfCorpus <- Corpus(articles_long, text_field = "word")
??Corpus
??Corpus()
dfCorpus <- Corpus(word, articles_long)
df$text <- gsub("<.*?>", "", df$text)
View(articles_long)
class(articles_long)
dfCorpus <- Corpus(articles_long, text_field = "word")
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(hunspell)
library(rvest)
library(stringr)
library(RSelenium)
base_url_pt1 <- "https://cryptonews.net/?page="
n_index_pages <- 25
get_article_list_from_page <- function(page_no) {
page_url <- paste0(base_url_pt1, page_no)
page <- read_html(page_url)
links <- page %>%
html_node("section") %>%
html_nodes("div.desc.col-xs")
articles_tmp <- tibble(link=paste0("https://cryptonews.net",url = links %>% html_node("a") %>% html_attr("href")),
title =  links %>% html_node("a") %>% html_text())
return(articles_tmp)
}
article_links <- tibble()
for(i in 25:n_index_pages) {
article_links_tmp <- get_article_list_from_page(i)
article_links <- bind_rows(article_links, article_links_tmp)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
print(i)
}
rm(article_links_tmp, i)
webpage <- read_html(encoding = "ISO_8859-2", 'https://cryptonews.net/news/other/21113274/')
webpage
results <- webpage %>% html_nodes(".short-desc")
results
safe_read_html <- safely(read_html)
get_article <- function(art_url) {
try(page <- safe_read_html(art_url, encoding = "iso-8859-2"))
if(is.null(page$result)) {
return(tibble())
}
page <- page$result
author <- page %>% html_node('span.source-host') %>% html_text() %>% trimws()
date <- page %>% html_node("span.datetime.flex.middle-xs") %>% html_text() %>% trimws() %>% dmy_hm()
title <- page %>%html_node("h1") %>% html_text() %>% trimws()
lead <- page %>% html_node("div.news-item.detail.content_text") %>% html_node("p") %>% html_text() %>% trimws()
body <- page %>% html_node("div.news-item.detail.content_text") %>% html_nodes("p") %>% html_text() %>% trimws()
article <- tibble(url = art_url, title = title, author = author, date = date, lead = lead, body = body,)
Sys.sleep(sample(seq(0.25, 1, 0.5), 1))
return(article)
}
articles <- article_links %>%
rowwise() %>%
do(get_article(.$link)) %>%
bind_rows() %>%
ungroup()
articles_grouped <- articles %>%
group_by(url,title,author,date,lead )  %>%
summarise(full_body = paste(body, collapse = " "))
articles_long <- articles_grouped["full_body"] %>%
unnest_tokens(word,full_body)
dfCorpus <- Corpus(articles_long, text_field = "word")
dfCorpus <- koRpus::corpus(articles_long, text_field = "word")
dfCorpus <- tm::Corpus(articles_long, text_field = "word")
dfCorpus <- textstem::::Corpus(articles_long, text_field = "word")
dfCorpus <- textstem::Corpus(articles_long, text_field = "word")
dfCorpus <- textstem::corpus(articles_long, text_field = "word")
dfCorpus <- corpus(articles_long, text_field = "word")
class(articles_long)
View(articles_long)
View(articles_long)
dfCorpus <- Corpus(articles_long, text_field = "word")
??Corpis
??Corpus()
dfCorpus <- Corpus(articles_long)
dfCorpus <- Corpus(articles_long, "word")
dfCorpus <- Corpus()
dfCorpus <- Corpus(articles_long,
text_field = "word",
unique_docnames = TRUE,)
dfCorpus <- Corpus(articles_long,
text_field = 1,
unique_docnames = TRUE,)
dfCorpus <- Corpus(articles_long,
text_field = 1)
dfCorpus <- Corpus(articles_long,
text_field = word)
dfCorpus <- Corpus(articles_long,
text_field = "word")
articles_long[1][1]
articles_long[1,1]
dfCorpus <- Corpus(VectorSource(article_long)
dfCorpus <- Corpus(VectorSource(articles_long))
dfCorpus <- Corpus(VectorSource(articles_long))
View(dfCorpus)
inspect(articlesCorpus[1])
articlesCorpus <- Corpus(VectorSource(articles_long))
inspect(articlesCorpus[1])
clean.corpus <- function(corpus){
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords('en'))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stemDocument)
return(corpus)
}
articlesCorpus <-  clean.corpus(articlesCorpus)
inspect(articlesCorpus[1])
articlesCorpus <- Corpus(VectorSource(articles_long))
inspect(articlesCorpus[1])
articlesCorpus <-  clean.corpus(articlesCorpus)
inspect(articlesCorpus[1])
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]')) %>%
filter(str_detect(word, '[A-z]'))
articles_long <- articles_long %>%
mutate(word2 = hunspell_stem(word))
articles_long$word2 <- sapply(articles_long$word2, function(x) x[1])
articles_long <- articles_long %>%
mutate(word3 = ifelse(is.na(word2), word, word2))
articles_long <- articles_grouped["full_body"] %>%
unnest_tokens(word,full_body)
anti_join(stop_words, by = 'word') %>%
filter(!str_detect(word, '[0-9]'))
articles_long <- articles_long %>%
mutate(word2 = hunspell_stem(word))
articles_long$word2 <- sapply(articles_long$word2, function(x) x[1])
articles_long <- articles_long %>%
mutate(word3 = ifelse(is.na(word2), word, word2))
articles_long <- articles_grouped["full_body"] %>%
unnest_tokens(word,full_body)
articlesCorpus <- Corpus(VectorSource(articles_long))
clean.corpus <- function(corpus){
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords('en'))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stemDocument)
return(corpus)
}
articlesCorpus <-  clean.corpus(articlesCorpus)
inspect(articlesCorpus[1])
View(articlesCorpus)
inspect(articlesCorpus[1])
